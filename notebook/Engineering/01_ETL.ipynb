{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1deb2a6-54c1-442b-8f09-3cd3d360e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 50 kB/s  eta 0:00:01     |██████████████████              | 1.7 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: python-dotenv, psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10 python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508a7f35-f7f7-4f1e-b9a9-d3b106b88adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo tabela pequena: clientes\n",
      "✅ Salvou: data/clientes.csv (1 linhas)\n",
      "✅ Extração finalizada.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load configuração do YAML\n",
    "with open('tables.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Cria pasta 'data' se não existir\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Configurações de conexão com o PostgreSQL\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"admin\"\n",
    "HOST = \"192.168.0.202\"\n",
    "PORT = \"5432\"\n",
    "DB = \"postgres\"\n",
    "\n",
    "# Cria engine SQLAlchemy\n",
    "engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\")\n",
    "\n",
    "def extrair_tabela_pequena(source, target):\n",
    "    print(f\"Extraindo tabela pequena: {source}\")\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {source}\", engine)\n",
    "    output_path = f\"data/{target}.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Salvou: {output_path} ({len(df)} linhas)\")\n",
    "\n",
    "def extrair_tabela_grande(source, target, partition_config):\n",
    "    col = partition_config[\"column\"]\n",
    "    lower = partition_config[\"lower_bound\"]\n",
    "    upper = partition_config[\"upper_bound\"]\n",
    "    num_parts = partition_config[\"num_partitions\"]\n",
    "\n",
    "    step = (upper - lower + 1) // num_parts\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        start = lower + i * step\n",
    "        end = start + step - 1 if i < num_parts - 1 else upper\n",
    "\n",
    "        print(f\"Extraindo {target}_{i+1:02d}: {col} de {start} até {end}\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT * FROM {source}\n",
    "            WHERE {col} BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(query, engine, params=(start, end))\n",
    "        output_path = f\"data/{target}_{i+1:02d}.csv\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"✅ Salvou: {output_path} ({len(df)} linhas)\")\n",
    "\n",
    "# Loop pelas tabelas do YAML\n",
    "for table in config['folders']:\n",
    "    source = table['source']\n",
    "    target = table['target']\n",
    "    large = table.get('large_table', False)\n",
    "\n",
    "    if large:\n",
    "        extrair_tabela_grande(source, target, table['partition_config'])\n",
    "    else:\n",
    "        extrair_tabela_pequena(source, target)\n",
    "\n",
    "print(\"✅ Extração finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b8eb1-2d53-45e7-a06f-8c4c1f960eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8cf566-ad03-49f8-a9c0-2d60391985ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo clientes_01: id de 1 até 100\n",
      "✅ Salvou: data/clientes_01.csv (1 linhas)\n",
      "Extraindo clientes_02: id de 101 até 200\n",
      "✅ Salvou: data/clientes_02.csv (0 linhas)\n",
      "Extraindo clientes_03: id de 201 até 300\n",
      "✅ Salvou: data/clientes_03.csv (0 linhas)\n",
      "Extraindo clientes_04: id de 301 até 400\n",
      "✅ Salvou: data/clientes_04.csv (0 linhas)\n",
      "Extraindo clientes_05: id de 401 até 500\n",
      "✅ Salvou: data/clientes_05.csv (0 linhas)\n",
      "Extraindo clientes_06: id de 501 até 600\n",
      "✅ Salvou: data/clientes_06.csv (0 linhas)\n",
      "Extraindo clientes_07: id de 601 até 700\n",
      "✅ Salvou: data/clientes_07.csv (0 linhas)\n",
      "Extraindo clientes_08: id de 701 até 800\n",
      "✅ Salvou: data/clientes_08.csv (0 linhas)\n",
      "Extraindo clientes_09: id de 801 até 900\n",
      "✅ Salvou: data/clientes_09.csv (0 linhas)\n",
      "Extraindo clientes_10: id de 901 até 1000\n",
      "✅ Salvou: data/clientes_10.csv (0 linhas)\n",
      "✅ Extração finalizada.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load configuração do YAML\n",
    "with open('tables.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Cria pasta 'data' se não existir\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Configurações de conexão com o PostgreSQL\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"admin\"\n",
    "HOST = \"192.168.0.202\"\n",
    "PORT = \"5432\"\n",
    "DB = \"postgres\"\n",
    "\n",
    "# Cria engine SQLAlchemy\n",
    "engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\")\n",
    "\n",
    "def extrair_tabela_pequena(source, target):\n",
    "    print(f\"Extraindo tabela pequena: {source}\")\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {source}\", engine)\n",
    "    output_path = f\"data/{target}.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Salvou: {output_path} ({len(df)} linhas)\")\n",
    "\n",
    "def extrair_tabela_grande(source, target, table_name, partition_config):\n",
    "    col = partition_config[\"column\"]\n",
    "    lower = partition_config[\"lower_bound\"]\n",
    "    upper = partition_config[\"upper_bound\"]\n",
    "    num_parts = partition_config[\"num_partitions\"]\n",
    "\n",
    "    step = (upper - lower + 1) // num_parts\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        start = lower + i * step\n",
    "        end = start + step - 1 if i < num_parts - 1 else upper\n",
    "\n",
    "        print(f\"Extraindo {target}_{i+1:02d}: {col} de {start} até {end}\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            WITH tb_auditoria AS (\n",
    "                SELECT ID_REGISTRO, NOME_TABELA, data,\n",
    "                       ROW_NUMBER() OVER (PARTITION BY ID_REGISTRO ORDER BY DATA DESC) AS RN\n",
    "                FROM PUBLIC.AUDITORIA\n",
    "                WHERE 1=1\n",
    "                    AND NOME_TABELA = %s\n",
    "                    AND data >= CURRENT_DATE - 1\n",
    "                    AND data <= CURRENT_DATE\n",
    "                    AND ID_REGISTRO BETWEEN %s AND %s\n",
    "            )\n",
    "            SELECT c.*\n",
    "            FROM tb_auditoria audit\n",
    "            INNER JOIN {source} c ON c.id = audit.ID_REGISTRO\n",
    "            WHERE audit.RN = 1\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(query, engine, params=(table_name, start, end))\n",
    "        output_path = f\"data/{target}_{i+1:02d}.csv\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"✅ Salvou: {output_path} ({len(df)} linhas)\")\n",
    "\n",
    "# Loop pelas tabelas do YAML\n",
    "for table in config['folders']:\n",
    "    source = table['source']\n",
    "    target = table['target']\n",
    "    large = table.get('large_table', False)\n",
    "\n",
    "    if large:\n",
    "        table_name = table.get('table_name', target)  # Usa 'target' como fallback se 'table_name' não estiver definido\n",
    "        extrair_tabela_grande(source, target, table_name, table['partition_config'])\n",
    "    else:\n",
    "        extrair_tabela_pequena(source, target)\n",
    "\n",
    "print(\"✅ Extração finalizada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5707a5-6651-4756-9f08-aff2ad975e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo clientes_01: id de 1 até 100\n",
      "✅ Salvou: data/clientes_01.csv (98 linhas)\n",
      "Extraindo clientes_02: id de 101 até 200\n",
      "✅ Salvou: data/clientes_02.csv (100 linhas)\n",
      "Extraindo clientes_03: id de 201 até 300\n",
      "✅ Salvou: data/clientes_03.csv (100 linhas)\n",
      "Extraindo clientes_04: id de 301 até 400\n",
      "✅ Salvou: data/clientes_04.csv (100 linhas)\n",
      "Extraindo clientes_05: id de 401 até 500\n",
      "✅ Salvou: data/clientes_05.csv (100 linhas)\n",
      "Extraindo clientes_06: id de 501 até 600\n",
      "✅ Salvou: data/clientes_06.csv (100 linhas)\n",
      "Extraindo clientes_07: id de 601 até 700\n",
      "✅ Salvou: data/clientes_07.csv (100 linhas)\n",
      "Extraindo clientes_08: id de 701 até 800\n",
      "✅ Salvou: data/clientes_08.csv (100 linhas)\n",
      "Extraindo clientes_09: id de 801 até 900\n",
      "✅ Salvou: data/clientes_09.csv (100 linhas)\n",
      "Extraindo clientes_10: id de 901 até 1000\n",
      "✅ Salvou: data/clientes_10.csv (100 linhas)\n",
      "✅ Extração finalizada.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load configuração do YAML\n",
    "with open('tables.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Cria pasta 'data' se não existir\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Configurações de conexão com o PostgreSQL\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"admin\"\n",
    "HOST = \"192.168.0.202\"\n",
    "PORT = \"5432\"\n",
    "DB = \"postgres\"\n",
    "\n",
    "# Cria engine SQLAlchemy\n",
    "engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\")\n",
    "\n",
    "def extrair_tabela_pequena(source, target, table_name):\n",
    "    print(f\"Extraindo tabela pequena: {source}\")\n",
    "    query = f\"\"\"\n",
    "        WITH tb_auditoria AS (\n",
    "            SELECT ID_REGISTRO, NOME_TABELA, data,\n",
    "                   ROW_NUMBER() OVER (PARTITION BY ID_REGISTRO ORDER BY DATA DESC) AS RN\n",
    "            FROM PUBLIC.AUDITORIA\n",
    "            WHERE 1=1\n",
    "                AND NOME_TABELA = %s\n",
    "                AND data >= CURRENT_DATE - 1\n",
    "                AND data <= CURRENT_DATE\n",
    "        )\n",
    "        SELECT c.*\n",
    "        FROM tb_auditoria audit\n",
    "        INNER JOIN {source} c ON c.id = audit.ID_REGISTRO\n",
    "        WHERE audit.RN = 1\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, engine, params=(table_name,))\n",
    "    output_path = f\"data/{target}.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Salvou: {output_path} ({len(df)} linhas)\")\n",
    "\n",
    "def extrair_tabela_grande(source, target, table_name, partition_config):\n",
    "    col = partition_config[\"column\"]\n",
    "    lower = partition_config[\"lower_bound\"]\n",
    "    upper = partition_config[\"upper_bound\"]\n",
    "    num_parts = partition_config[\"num_partitions\"]\n",
    "\n",
    "    step = (upper - lower + 1) // num_parts\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        start = lower + i * step\n",
    "        end = start + step - 1 if i < num_parts - 1 else upper\n",
    "\n",
    "        print(f\"Extraindo {target}_{i+1:02d}: {col} de {start} até {end}\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            WITH tb_auditoria AS (\n",
    "                SELECT ID_REGISTRO, NOME_TABELA, data,\n",
    "                       ROW_NUMBER() OVER (PARTITION BY ID_REGISTRO ORDER BY DATA DESC) AS RN\n",
    "                FROM PUBLIC.AUDITORIA\n",
    "                WHERE 1=1\n",
    "                    AND NOME_TABELA = %s\n",
    "                    AND data >= CURRENT_DATE - 1\n",
    "                    AND data <= CURRENT_DATE\n",
    "                    AND ID_REGISTRO BETWEEN %s AND %s\n",
    "            )\n",
    "            SELECT c.*\n",
    "            FROM tb_auditoria audit\n",
    "            INNER JOIN {source} c ON c.id = audit.ID_REGISTRO\n",
    "            WHERE audit.RN = 1\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(query, engine, params=(table_name, start, end))\n",
    "        output_path = f\"data/{target}_{i+1:02d}.csv\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"✅ Salvou: {output_path} ({len(df)} linhas)\")\n",
    "\n",
    "# Loop pelas tabelas do YAML\n",
    "for table in config['folders']:\n",
    "    source = table['source']\n",
    "    target = table['target']\n",
    "    table_name = table.get('table_name', target)  # Usa 'target' como fallback\n",
    "    large = table.get('large_table', False)\n",
    "\n",
    "    if large:\n",
    "        extrair_tabela_grande(source, target, table_name, table['partition_config'])\n",
    "    else:\n",
    "        extrair_tabela_pequena(source, target, table_name)\n",
    "\n",
    "print(\"✅ Extração finalizada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a61e9b-de44-4112-a9a9-e5364c25df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\": \"INFO\", \"message\": \"Limites de id para clientes: 1 até 6110003\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Esperado 100000 linhas para public.clientes (id de 1 até 6110003)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Particionando clientes: id de 1 até 6110003 em 20 partições\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_01: id de 1 até 305500\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_01.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_01.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_02: id de 305501 até 611000\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_02.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_02.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_03: id de 611001 até 916500\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_03.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_03.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_04: id de 916501 até 1222000\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_04.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_04.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_05: id de 1222001 até 1527500\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_05.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_05.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_06: id de 1527501 até 1833000\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_06.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_06.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_07: id de 1833001 até 2138500\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_07.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_07.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_08: id de 2138501 até 2444000\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_08.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_08.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_09: id de 2444001 até 2749500\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_09.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_09.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_10: id de 2749501 até 3055000\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_10.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_10.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_11: id de 3055001 até 3360500\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_11.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_11.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_12: id de 3360501 até 3666000\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_12.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_12.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_13: id de 3666001 até 3971500\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_13.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_13.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_14: id de 3971501 até 4277000\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_14.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_14.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_15: id de 4277001 até 4582500\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_15.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_15.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_16: id de 4582501 até 4888000\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_16.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_16.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_17: id de 4888001 até 5193500\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_17.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_17.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_18: id de 5193501 até 5499000\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_18.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_18.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_19: id de 5499001 até 5804500\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 0 linhas para data/clientes_19.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_19.csv (0 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Extraindo clientes_20: id de 5804501 até 6110003\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Processado chunk de 100000 linhas para data/clientes_20.csv\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Salvou: data/clientes_20.csv (100000 linhas)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"Total extraído para clientes: 100000 linhas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"✅ Extração finalizada.\"}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# Configuração do Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='{\"level\": \"%(levelname)s\", \"message\": \"%(message)s\"}',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(\"etl_postgres\")\n",
    "\n",
    "# Load configuração do YAML\n",
    "with open('tables.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Cria pasta 'data' se não existir\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Configurações de conexão com o PostgreSQL\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"admin\"\n",
    "HOST = \"192.168.0.202\"\n",
    "PORT = \"5432\"\n",
    "DB = \"postgres\"\n",
    "\n",
    "# Cria engine SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\",\n",
    "    connect_args={'connect_timeout': 300}\n",
    ")\n",
    "\n",
    "def get_id_bounds(table_name, date_filter_days=1):\n",
    "    query = \"\"\"\n",
    "        SELECT MIN(ID_REGISTRO), MAX(ID_REGISTRO)\n",
    "        FROM PUBLIC.AUDITORIA\n",
    "        WHERE NOME_TABELA = %s\n",
    "          AND data >= CURRENT_DATE - %s\n",
    "          AND data <= CURRENT_DATE\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = pd.read_sql_query(query, engine, params=(table_name, date_filter_days))\n",
    "        min_id = result.iloc[0]['min']\n",
    "        max_id = result.iloc[0]['max']\n",
    "        # Converter numpy.int64 para int Python\n",
    "        min_id = int(min_id) if min_id is not None else 1\n",
    "        max_id = int(max_id) if max_id is not None else 1\n",
    "        return min_id, max_id\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Erro ao obter limites de ID para {table_name}: {str(e)}\")\n",
    "        return 1, 1\n",
    "\n",
    "def contar_registros_esperados(source, table_name, date_filter_days=1, lower=None, upper=None):\n",
    "    query = \"\"\"\n",
    "        WITH tb_auditoria AS (\n",
    "            SELECT ID_REGISTRO, NOME_TABELA, data,\n",
    "                   ROW_NUMBER() OVER (PARTITION BY ID_REGISTRO ORDER BY DATA DESC) AS RN\n",
    "            FROM PUBLIC.AUDITORIA\n",
    "            WHERE 1=1\n",
    "                AND NOME_TABELA = %s\n",
    "                AND data >= CURRENT_DATE - %s\n",
    "                AND data <= CURRENT_DATE\n",
    "    \"\"\"\n",
    "    params = [table_name, date_filter_days]\n",
    "    if lower is not None and upper is not None:\n",
    "        query += \" AND ID_REGISTRO BETWEEN %s AND %s\"\n",
    "        # Garantir que lower e upper sejam int\n",
    "        params.extend([int(lower), int(upper)])\n",
    "    query += \"\"\"\n",
    "        )\n",
    "        SELECT COUNT(*)\n",
    "        FROM tb_auditoria audit\n",
    "        INNER JOIN {} c ON c.id = audit.ID_REGISTRO\n",
    "        WHERE audit.RN = 1\n",
    "    \"\"\".format(source)\n",
    "    \n",
    "    try:\n",
    "        count = pd.read_sql_query(query, engine, params=tuple(params))['count'].iloc[0]\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Erro ao contar registros para {source}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def extrair_tabela_pequena(source, target, table_name, date_filter_days=1):\n",
    "    logger.info(f\"Extraindo tabela pequena: {source}\")\n",
    "    \n",
    "    # Contar registros esperados\n",
    "    expected_rows = contar_registros_esperados(source, table_name, date_filter_days)\n",
    "    logger.info(f\"Esperado {expected_rows} linhas para {source}\")\n",
    "\n",
    "    query = \"\"\"\n",
    "        WITH tb_auditoria AS (\n",
    "            SELECT ID_REGISTRO, NOME_TABELA, data,\n",
    "                   ROW_NUMBER() OVER (PARTITION BY ID_REGISTRO ORDER BY DATA DESC) AS RN\n",
    "            FROM PUBLIC.AUDITORIA\n",
    "            WHERE 1=1\n",
    "                AND NOME_TABELA = %s\n",
    "                AND data >= CURRENT_DATE - %s\n",
    "                AND data <= CURRENT_DATE\n",
    "        )\n",
    "        SELECT c.*\n",
    "        FROM tb_auditoria audit\n",
    "        INNER JOIN {} c ON c.id = audit.ID_REGISTRO\n",
    "        WHERE audit.RN = 1\n",
    "    \"\"\".format(source)\n",
    "    \n",
    "    output_path = f\"data/{target}.csv\"\n",
    "    total_rows = 0\n",
    "    first_chunk = True\n",
    "\n",
    "    try:\n",
    "        for chunk in pd.read_sql_query(query, engine, params=(table_name, date_filter_days), chunksize=100000):\n",
    "            mode = 'w' if first_chunk else 'a'\n",
    "            header = first_chunk\n",
    "            chunk.to_csv(output_path, mode=mode, header=header, index=False)\n",
    "            total_rows += len(chunk)\n",
    "            logger.info(f\"Processado chunk de {len(chunk)} linhas para {output_path}\")\n",
    "            first_chunk = False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Erro ao processar tabela pequena {source}: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"✅ Salvou: {output_path} ({total_rows} linhas)\")\n",
    "    if total_rows != expected_rows:\n",
    "        logger.warning(f\"⚠️ Discrepância: Extraído {total_rows} linhas, esperado {expected_rows}\")\n",
    "\n",
    "def extrair_tabela_grande(source, target, table_name, partition_config):\n",
    "    col = partition_config[\"column\"]\n",
    "    num_parts = partition_config.get(\"num_partitions\", 100)  # Padrão: 100 partições\n",
    "    date_filter_days = partition_config.get(\"date_filter_days\", 1)  # Padrão: 1 dia\n",
    "\n",
    "    # Obter limites dinâmicos de ID_REGISTRO\n",
    "    lower, upper = get_id_bounds(table_name, date_filter_days)\n",
    "    logger.info(f\"Limites de {col} para {table_name}: {lower} até {upper}\")\n",
    "\n",
    "    # Contar registros esperados\n",
    "    expected_rows = contar_registros_esperados(source, table_name, date_filter_days, lower, upper)\n",
    "    logger.info(f\"Esperado {expected_rows} linhas para {source} ({col} de {lower} até {upper})\")\n",
    "\n",
    "    # Ajustar número de partições se o intervalo for pequeno\n",
    "    if upper - lower + 1 < num_parts:\n",
    "        num_parts = max(1, int(upper - lower + 1))\n",
    "    step = (upper - lower + 1) // num_parts\n",
    "    logger.info(f\"Particionando {target}: {col} de {lower} até {upper} em {num_parts} partições\")\n",
    "\n",
    "    total_rows_all = 0\n",
    "    for i in range(num_parts):\n",
    "        start = lower + i * step\n",
    "        end = start + step - 1 if i < num_parts - 1 else upper\n",
    "\n",
    "        logger.info(f\"Extraindo {target}_{i+1:02d}: {col} de {start} até {end}\")\n",
    "\n",
    "        query = \"\"\"\n",
    "            WITH tb_auditoria AS (\n",
    "                SELECT ID_REGISTRO, NOME_TABELA, data,\n",
    "                       ROW_NUMBER() OVER (PARTITION BY ID_REGISTRO ORDER BY DATA DESC) AS RN\n",
    "                FROM PUBLIC.AUDITORIA\n",
    "                WHERE 1=1\n",
    "                    AND NOME_TABELA = %s\n",
    "                    AND data >= CURRENT_DATE - %s\n",
    "                    AND data <= CURRENT_DATE\n",
    "                    AND ID_REGISTRO BETWEEN %s AND %s\n",
    "            )\n",
    "            SELECT c.*\n",
    "            FROM tb_auditoria audit\n",
    "            INNER JOIN {} c ON c.id = audit.ID_REGISTRO\n",
    "            WHERE audit.RN = 1\n",
    "        \"\"\".format(source)\n",
    "\n",
    "        output_path = f\"data/{target}_{i+1:02d}.csv\"\n",
    "        total_rows = 0\n",
    "        first_chunk = True\n",
    "\n",
    "        try:\n",
    "            for chunk in pd.read_sql_query(query, engine, params=(table_name, date_filter_days, int(start), int(end)), chunksize=100000):\n",
    "                mode = 'w' if first_chunk else 'a'\n",
    "                header = first_chunk\n",
    "                chunk.to_csv(output_path, mode=mode, header=header, index=False)\n",
    "                total_rows += len(chunk)\n",
    "                logger.info(f\"Processado chunk de {len(chunk)} linhas para {output_path}\")\n",
    "                first_chunk = False\n",
    "            total_rows_all += total_rows\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Erro ao processar partição {target}_{i+1:02d}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"✅ Salvou: {output_path} ({total_rows} linhas)\")\n",
    "\n",
    "    logger.info(f\"Total extraído para {target}: {total_rows_all} linhas\")\n",
    "    if total_rows_all != expected_rows:\n",
    "        logger.warning(f\"⚠️ Discrepância: Extraído {total_rows_all} linhas, esperado {expected_rows}\")\n",
    "\n",
    "# Loop pelas tabelas do YAML\n",
    "for table in config['folders']:\n",
    "    source = table['source']\n",
    "    target = table['target']\n",
    "    table_name = table.get('table_name', target)  # Usa 'target' como fallback\n",
    "    large = table.get('large_table', False)\n",
    "\n",
    "    try:\n",
    "        if large:\n",
    "            extrair_tabela_grande(source, target, table_name, table['partition_config'])\n",
    "        else:\n",
    "            date_filter_days = table.get('date_filter_days', 1)\n",
    "            extrair_tabela_pequena(source, target, table_name, date_filter_days)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Erro ao processar tabela {source}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "logger.info(\"✅ Extração finalizada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3465cc1-bb16-43e2-a37a-eada0b37ec56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
