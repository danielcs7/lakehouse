version: "3.0"

services:

  airflow-init:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create --username airflow --firstname Air --lastname Flow --role Admin --email airflow@example.com --password airflow
      "
    volumes:
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
 
  airflow-webserver:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-webserver
    depends_on:
      - postgres
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    volumes:
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command: webserver
    restart: always

  airflow-scheduler:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
    volumes:
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
    command: scheduler
    restart: always

  # Storages
  postgres:
    container_name: postgres
    hostname: postgres
    image: postgres:11
    ports:
      - "5434:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./src/postgres/init-database.sh:/docker-entrypoint-initdb.d/init-database.sh
      - ~/projects/postgres-data:/var/lib/postgresql/data  # Volume mapeado para o SSD interno
      #- ./src/postgres/init-database.sh:/docker-entrypoint-initdb.d/init-database.sh
      #- ./src/volume/postgres:/var/lib/postgresql/data

    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "postgres" ]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  trino:
    container_name: trino
    hostname: trino
    image: "trinodb/trino:425"
    restart: always
    ports:
      - "8889:8889"
    volumes:
      - ./src/trino/etc-coordinator:/etc/trino
      - ./src/trino/catalog:/etc/trino/catalog
    depends_on:
      - hive-metastore

  trino-worker:
    profiles: [ "trino-worker" ]
    container_name: trino-worker
    hostname: trino-worker
    image: "trinodb/trino:425"
    restart: always
    volumes:
      - ./src/trino/etc-worker:/etc/trino
      - ./src/trino/catalog:/etc/trino/catalog
    depends_on:
      - trino

  minio:
    container_name: minio
    hostname: minio
    image: 'minio/minio'
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
    - .src/minio_data:/data  
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_KEY}
      MINIO_DOMAIN: ${MINIO_DOMAIN}
    command: server /data --console-address ":9001"

  minio-job:
    image: 'minio/mc'
    container_name: minio-job
    hostname: minio-job
    env_file:
      - .env
    entrypoint: |
      /bin/bash -c "
      sleep 5;
      /usr/bin/mc config --quiet host add myminio http://minio:9000 \$S3_ACCESS_KEY \$S3_SECRET_KEY || true;
      /usr/bin/mc mb --quiet myminio/datalake || true;
      "
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
      AWS_REGION: ${AWS_REGION}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_PATH_STYLE_ACCESS: "true"
    depends_on:
      - minio

  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    build:
      dockerfile: ./src/hive-metastore/Dockerfile
    image: dataincode/openlakehouse:hive-metastore-3.1.2
    env_file:
      - .env
    ports:
      - '9083:9083' # Metastore Thrift
    environment:
      HIVE_METASTORE_DRIVER: org.postgresql.Driver
      HIVE_METASTORE_JDBC_URL: ${HIVE_METASTORE_JDBC_URL}
      HIVE_METASTORE_USER: hive #${HIVE_METASTORE_USER}
      HIVE_METASTORE_PASSWORD: hive #${HIVE_METASTORE_PASSWORD}
      HIVE_METASTORE_WAREHOUSE_DIR: ${HIVE_METASTORE_WAREHOUSE_DIR}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      S3_PATH_STYLE_ACCESS: "true"
    depends_on:
      postgres:
        condition: service_healthy

  # spark-iceberg

  spark-iceberg:
    build:
      dockerfile: ./src/spark/Dockerfile-spark3.5
    image: dataincode/openlakehouse:spark-3.5
    container_name: spark-iceberg
    hostname: spark-iceberg
    entrypoint: |
      /bin/bash -c "
      jupyter lab --notebook-dir=/opt/notebook --ip='*' --NotebookApp.token='' --NotebookApp.password='' --port=8888 --no-browser --allow-root
      "
    ports:
      - "4040:4040"
      - "8900:8888"
    depends_on:
      - minio
      - hive-metastore
    environment:
      SPARK_MODE: master
      SPARK_MASTER_MEMORY: 4g
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
      AWS_REGION: ${AWS_REGION}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_PATH_STYLE_ACCESS: "true"

    volumes:
      - ./notebook:/opt/notebook
      - ./src/jupyter/jupyter_server_config.py:/root/.jupyter/jupyter_server_config.py
      - ./src/jupyter/themes.jupyterlab-settings:/root/.jupyter/lab/user-settings/@jupyterlab/apputils-extension/themes.jupyterlab-settings
      - ./src/spark/spark-defaults-iceberg.conf:/opt/spark/conf/spark-defaults.conf
      - ./src/spark/spark-env.sh:/opt/spark/conf/spark-env.sh


# Configure Network
networks:
  default:
    name: iceber-net
